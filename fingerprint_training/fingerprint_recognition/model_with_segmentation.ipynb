{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import os\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "# from imgaug import augmenters as iaa\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Subtract, Conv2DTranspose, concatenate, Cropping2D\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Here, we are using numpy as Keras is used with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Dataset\n",
    "def load_dataset():\n",
    "    datasets = ['real', 'easy', 'medium', 'hard']\n",
    "    data_dict = {}\n",
    "    for dataset in datasets:\n",
    "        x_data = np.load(f'dataset/x_{dataset}.npy')\n",
    "        y_data = np.load(f'dataset/y_{dataset}.npy')\n",
    "        data_dict[dataset] = (x_data, y_data)\n",
    "        print(f\"{dataset}: X shape {x_data.shape}, Y shape {y_data.shape}\")\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "data_dict = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segmentation_dataset():\n",
    "    \"\"\"\n",
    "    Load the fingerprint images and their corresponding mask annotations.\n",
    "    If the dataset doesn't exist, create synthetic masks for demonstration.\n",
    "    \"\"\"\n",
    "    # For demonstration purposes, we'll create synthetic masks\n",
    "    # In a real scenario, you would load actual annotated masks\n",
    "    \n",
    "    print(\"Loading fingerprint region segmentation dataset...\")\n",
    "    \n",
    "    # Try to load existing segmentation dataset if available\n",
    "    try:\n",
    "        x_images = np.load('dataset/x_segmentation.npy')\n",
    "        y_masks = np.load('dataset/y_segmentation.npy')\n",
    "        print(f\"Loaded existing segmentation dataset: {x_images.shape}, {y_masks.shape}\")\n",
    "        return x_images, y_masks\n",
    "    except:\n",
    "        print(\"Creating synthetic segmentation dataset...\")\n",
    "    \n",
    "    # Use the existing fingerprint images\n",
    "    data_dict = load_dataset()\n",
    "    \n",
    "    # Combine images from different difficulty levels\n",
    "    all_images = np.concatenate([\n",
    "        data_dict['real'][0],\n",
    "        data_dict['easy'][0],\n",
    "        data_dict['medium'][0],\n",
    "        data_dict['hard'][0]\n",
    "    ], axis=0)\n",
    "    \n",
    "    # Create synthetic masks (in a real scenario, these would be manually annotated)\n",
    "    masks = []\n",
    "    for img in all_images:\n",
    "        # Create a synthetic mask based on pixel intensity\n",
    "        # This assumes fingerprints are darker than the background\n",
    "        img_gray = img.squeeze()\n",
    "        \n",
    "        # Simple thresholding to create a binary mask\n",
    "        _, binary = cv2.threshold(img_gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        # Clean up the mask with morphological operations\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Normalize to 0-1\n",
    "        mask = binary / 255.0\n",
    "        masks.append(mask)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    x_images = all_images\n",
    "    y_masks = np.array(masks)[..., np.newaxis]\n",
    "    \n",
    "    # Save the dataset for future use\n",
    "    os.makedirs('dataset', exist_ok=True)\n",
    "    np.save('dataset/x_segmentation.npy', x_images)\n",
    "    np.save('dataset/y_segmentation.npy', y_masks)\n",
    "    \n",
    "    print(f\"Created segmentation dataset: X shape {x_images.shape}, Y shape {y_masks.shape}\")\n",
    "    return x_images, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample from each dataset\n",
    "def visualize_samples(data_dict):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for idx, (key, (x, y)) in enumerate(data_dict.items(), start=1):\n",
    "        plt.subplot(1, 4, idx)\n",
    "        plt.title(y[0])\n",
    "        plt.imshow(x[0].squeeze(), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_masks(images, masks, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize sample images and their corresponding segmentation masks.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(range(len(images)), num_samples, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*num_samples))\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Original image\n",
    "        plt.subplot(num_samples, 3, i*3+1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(images[idx].squeeze(), cmap='gray')\n",
    "        \n",
    "        # Mask\n",
    "        plt.subplot(num_samples, 3, i*3+2)\n",
    "        plt.title('Segmentation Mask')\n",
    "        plt.imshow(masks[idx].squeeze(), cmap='gray')\n",
    "        \n",
    "        # Overlay\n",
    "        plt.subplot(num_samples, 3, i*3+3)\n",
    "        plt.title('Overlay')\n",
    "        overlay = np.zeros_like(images[idx].squeeze())\n",
    "        overlay = np.stack([overlay, masks[idx].squeeze()*255, overlay], axis=-1)\n",
    "        plt.imshow(images[idx].squeeze(), cmap='gray')\n",
    "        plt.imshow(overlay, alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class IoU(Metric):\n",
    "    def __init__(self, name='iou', **kwargs):\n",
    "        super(IoU, self).__init__(name=name, **kwargs)\n",
    "        self.intersection = self.add_weight(name='intersection', initializer='zeros')\n",
    "        self.union = self.add_weight(name='union', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "        intersection = tf.reduce_sum(y_true * y_pred)\n",
    "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "        \n",
    "        self.intersection.assign_add(intersection)\n",
    "        self.union.assign_add(union)\n",
    "        \n",
    "    def result(self):\n",
    "        return self.intersection / (self.union + 1e-6)  # Adding small epsilon to avoid division by zero\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.intersection.assign(0.0)\n",
    "        self.union.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.concatenate([data_dict['easy'][0], data_dict['medium'][0], data_dict['hard'][0]], axis=0)\n",
    "label_data = np.concatenate([data_dict['easy'][1], data_dict['medium'][1], data_dict['hard'][1]], axis=0)\n",
    "\n",
    "# we use 90/10 split\n",
    "x_train, x_val, label_train, label_val = train_test_split(x_data, label_data, test_size=0.1)\n",
    "print(f\"x_train: {x_train.shape}, label_train: {label_train.shape}\")\n",
    "print(f\"x_val: {x_val.shape}, label_val: {label_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Augmentation\n",
    "\n",
    "These are functions and transformations that will serve to transform and distort the original image that we will use for testing the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_augmentation(images):\n",
    "    augmented_images = []\n",
    "    \n",
    "    for _ in range(9):\n",
    "        img = images[0].copy()\n",
    "        \n",
    "        # Apply Gaussian blur\n",
    "        blur_strength = random.uniform(0, 0.5)\n",
    "        if blur_strength > 0:\n",
    "            blur_size = int(blur_strength * 10) * 2 + 1  # Must be odd number\n",
    "            img = cv2.GaussianBlur(img, (blur_size, blur_size), blur_strength)\n",
    "        \n",
    "        # Apply affine transformations\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Random scale\n",
    "        scale_x = random.uniform(0.9, 1.1)\n",
    "        scale_y = random.uniform(0.9, 1.1)\n",
    "        \n",
    "        # Random translation\n",
    "        tx = random.uniform(-0.1, 0.1) * w\n",
    "        ty = random.uniform(-0.1, 0.1) * h\n",
    "        \n",
    "        # Random rotation\n",
    "        angle = random.uniform(-30, 30)\n",
    "        \n",
    "        # Compute affine transform matrix\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        M[0, 0] *= scale_x\n",
    "        M[1, 1] *= scale_y\n",
    "        M[0, 2] += tx\n",
    "        M[1, 2] += ty\n",
    "        \n",
    "        # Apply affine transform\n",
    "        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "        \n",
    "        augmented_images.append(img)\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(2, 5, 1)\n",
    "    plt.title('Original')\n",
    "    plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "    for i, aug in enumerate(augmented_images):\n",
    "        plt.subplot(2, 5, i + 2)\n",
    "        plt.title(f'Aug {i + 1}')\n",
    "        plt.imshow(aug.squeeze(), cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Label Dictionary Lookup Table\n",
    "\n",
    "This is used later on, to find the original index of a randomly picked sample from the validation set in order to compare the images for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_real_dict = {''.join(y.astype(str)).zfill(6): i for i, y in enumerate(data_dict['real'][1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "This is necessary step in image processing using Keras, especially a big dataset that cannot fit into the memory during training. It is a class that produces batches of data that can feed into multiple cores right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, x, label, x_real, label_real_dict, batch_size=32, shuffle=True):\n",
    "        self.x = x\n",
    "        self.label = label\n",
    "        self.x_real = x_real\n",
    "        self.label_real_dict = label_real_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x1_batch = self.x[index * self.batch_size:(index + 1) * self.batch_size].copy()\n",
    "        label_batch = self.label[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        x2_batch = np.empty((self.batch_size, 90, 90, 1), dtype=np.float32)\n",
    "        y_batch = np.zeros((self.batch_size, 1), dtype=np.float32)\n",
    "        \n",
    "        # Apply augmentations if shuffle is True\n",
    "        if self.shuffle:\n",
    "            # Apply OpenCV augmentations as in previous example\n",
    "            for i in range(len(x1_batch)):\n",
    "                # Augmentation code as provided earlier\n",
    "                pass\n",
    "        \n",
    "        for i, l in enumerate(label_batch):\n",
    "            match_key = ''.join(l.astype(str)).zfill(6)\n",
    "            if random.random() > 0.5:\n",
    "                x2_batch[i] = self.x_real[self.label_real_dict[match_key]][..., np.newaxis]\n",
    "                y_batch[i] = 1.\n",
    "            else:\n",
    "                while True:\n",
    "                    unmatch_key, unmatch_idx = random.choice(list(self.label_real_dict.items()))\n",
    "                    if unmatch_key != match_key:\n",
    "                        x2_batch[i] = self.x_real[unmatch_idx][..., np.newaxis]\n",
    "                        break\n",
    "                y_batch[i] = 0.\n",
    "        \n",
    "        # Return data in a format TensorFlow expects\n",
    "        # Instead of returning [x1_batch, x2_batch], y_batch\n",
    "        # Create a tuple of inputs\n",
    "        x_input = (\n",
    "            x1_batch.astype(np.float32) / 255., \n",
    "            x2_batch.astype(np.float32) / 255.\n",
    "        )\n",
    "        \n",
    "        return x_input, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.x, self.label = shuffle(self.x, self.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "Here is the concept of the model:\n",
    "- 2 models with 2x Convolutional and Pooling layers, sharing weights\n",
    "- The outputs are subtracted, and the subtraction is fed to another Conv + Pooling layer\n",
    "- Then that is fed to a sigmoid for final classification if the 2 images are same or not\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_unet_model(input_shape=(90, 90, 1)):\n",
    "    \"\"\"\n",
    "    Build a U-Net model for image segmentation with proper shape handling.\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder (downsampling path)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)  # 90→45\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)  # 45→22\n",
    "    \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)  # 22→11\n",
    "    \n",
    "    # Bridge\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "    \n",
    "    # Decoder (upsampling path)\n",
    "    u5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)  # 11→22\n",
    "    u5 = concatenate([u5, c3])  # Both 22×22\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
    "    \n",
    "    u6 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c5)  # 22→44\n",
    "    # Crop c2 to match u6's shape if needed\n",
    "    diff_y = c2.shape[1] - u6.shape[1]  # 45-44=1\n",
    "    diff_x = c2.shape[2] - u6.shape[2]  # 45-44=1\n",
    "    if diff_y > 0 or diff_x > 0:\n",
    "        from tensorflow.keras.layers import Cropping2D\n",
    "        c2 = Cropping2D(cropping=((0, diff_y), (0, diff_x)))(c2)\n",
    "    u6 = concatenate([u6, c2])  # Both 44×44\n",
    "    c6 = Conv2D(32, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(32, (3, 3), activation='relu', padding='same')(c6)\n",
    "    \n",
    "    u7 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c6)  # 44→88\n",
    "    # Crop c1 to match u7's shape if needed\n",
    "    diff_y = c1.shape[1] - u7.shape[1]  # 90-88=2\n",
    "    diff_x = c1.shape[2] - u7.shape[2]  # 90-88=2\n",
    "    if diff_y > 0 or diff_x > 0:\n",
    "        from tensorflow.keras.layers import Cropping2D\n",
    "        c1 = Cropping2D(cropping=((0, diff_y), (0, diff_x)))(c1)\n",
    "    u7 = concatenate([u7, c1])  # Both 88×88\n",
    "    c7 = Conv2D(16, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(16, (3, 3), activation='relu', padding='same')(c7)\n",
    "    \n",
    "    # Output layer - match original input shape with cropping if needed\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
    "    if outputs.shape[1] != input_shape[0] or outputs.shape[2] != input_shape[1]:\n",
    "        from tensorflow.keras.layers import ZeroPadding2D\n",
    "        outputs = ZeroPadding2D(padding=((1, 1), (1, 1)))(outputs)  # 88→90\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'IoU'])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[IoU(name='IoU')]\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation(model, x_val, y_val, num_samples=5):\n",
    "    \"\"\"\n",
    "    Evaluate the segmentation model and visualize results.\n",
    "    \"\"\"\n",
    "    # Predict on validation data\n",
    "    y_pred = model.predict(x_val)\n",
    "    \n",
    "    # Calculate IoU scores\n",
    "    iou_scores = []\n",
    "    for i in range(len(y_val)):\n",
    "        true = y_val[i].squeeze()\n",
    "        pred = (y_pred[i].squeeze() > 0.5).astype(np.float32)\n",
    "        \n",
    "        intersection = np.sum(true * pred)\n",
    "        union = np.sum(true) + np.sum(pred) - intersection\n",
    "        iou = intersection / (union + 1e-7)\n",
    "        iou_scores.append(iou)\n",
    "    \n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    print(f\"Mean IoU on validation set: {mean_iou:.4f}\")\n",
    "    \n",
    "    # Visualize random samples\n",
    "    indices = np.random.choice(range(len(x_val)), num_samples, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*num_samples))\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Original image\n",
    "        plt.subplot(num_samples, 3, i*3+1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(x_val[idx].squeeze(), cmap='gray')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        plt.subplot(num_samples, 3, i*3+2)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.imshow(y_val[idx].squeeze(), cmap='gray')\n",
    "        \n",
    "        # Predicted mask\n",
    "        plt.subplot(num_samples, 3, i*3+3)\n",
    "        plt.title(f'Prediction (IoU: {iou_scores[idx]:.4f})')\n",
    "        plt.imshow(y_pred[idx].squeeze(), cmap='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def train_segmentation_model():\n",
    "    \"\"\"\n",
    "    Train a U-Net model for fingerprint region segmentation.\n",
    "    \"\"\"\n",
    "    # Load or create the segmentation dataset\n",
    "    x_images, y_masks = load_segmentation_dataset()\n",
    "    \n",
    "    # Normalize the images\n",
    "    x_images = x_images.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_images, y_masks, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Visualize some samples\n",
    "    visualize_masks(x_train, y_train, num_samples=3)\n",
    "    \n",
    "    # Create U-Net model\n",
    "    model = build_unet_model(input_shape=(90, 90, 1))\n",
    "    model.summary()\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_custom_iou',  # Match the metric name\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=5,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate the model on validation data\n",
    "    evaluate_segmentation(model, x_val, y_val)\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Subtract\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create the base network (twin network) for the Siamese architecture.\n",
    "    \"\"\"\n",
    "    input = Input(shape=input_shape)\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Flatten the feature map\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    return Model(input, x)\n",
    "\n",
    "# Define the input shape for the images\n",
    "input_shape = (90, 90, 1)\n",
    "\n",
    "# Create the twin networks (base networks with shared weights)\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "# Define two input tensors for the two images\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# Process each image through the shared twin network\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "# Compute the absolute difference between the feature vectors\n",
    "subtracted = Subtract()([processed_a, processed_b])\n",
    "\n",
    "# Further processing of the subtracted feature using another convolutional layer\n",
    "x = Dense(128, activation='relu')(subtracted)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer with sigmoid activation for binary classification\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the full Siamese network model\n",
    "model = Model(inputs=[input_a, input_b], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "In other versons of the solutions, they used fit_generator method, which was commonly used in older versions of Keras to train models using data generators. However, since TensorFlow 2.1, fit can handle data generators directly. This unifies the API, making the code cleaner and more consistent. It also allows for better integration with TensorFlow’s functionalities such as distributed training and callbacks. Hence, using fit instead of fit_generator is the recommended approach now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model, seg_history = train_segmentation_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(x_train, label_train, data_dict['real'][0], label_real_dict, shuffle=True)\n",
    "val_gen = DataGenerator(x_val, label_val, data_dict['real'][0], label_real_dict, shuffle=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=15,\n",
    "    validation_data=val_gen\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "This is just a test using one random image and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_fingerprint_region(image, seg_model, padding=10):\n",
    "#     # Ensure image has proper dimensions\n",
    "#     original_image = image.copy()\n",
    "    \n",
    "#     try:\n",
    "#         # Prepare image for the segmentation model\n",
    "#         if len(image.shape) == 2:\n",
    "#             # Add channel dimension if grayscale\n",
    "#             input_img = np.expand_dims(image, axis=-1)\n",
    "#         else:\n",
    "#             input_img = image\n",
    "            \n",
    "#         # Resize if necessary based on model's expected input shape\n",
    "#         # model_input_shape = seg_model.inputs[0].shape.as_list()[1:3]  \n",
    "        \n",
    "#         try:\n",
    "#         # Try TensorFlow tensor approach\n",
    "#             model_input_shape = seg_model.inputs[0].shape.as_list()[1:3]\n",
    "#         except AttributeError:\n",
    "#             # Handle case where shape is a tuple\n",
    "#             model_input_shape = seg_model.inputs[0].shape[1:3]\n",
    "#         # Get expected height and width\n",
    "#         if model_input_shape[0] is not None and model_input_shape[1] is not None:\n",
    "#             # If model expects specific dimensions, resize the image\n",
    "#             input_img_resized = cv2.resize(input_img, (model_input_shape[1], model_input_shape[0]))\n",
    "#             # Ensure proper dimensions for model input (batch dimension)\n",
    "#             input_img_resized = np.expand_dims(input_img_resized, axis=0)\n",
    "#         else:\n",
    "#             # If model accepts dynamic input, just add batch dimension\n",
    "#             input_img_resized = np.expand_dims(input_img, axis=0)\n",
    "        \n",
    "#         # Predict segmentation mask\n",
    "#         prediction = seg_model.predict(input_img_resized)\n",
    "        \n",
    "#         # Process the predicted mask\n",
    "#         if len(prediction.shape) == 4:  # (batch, height, width, channels)\n",
    "#             mask = prediction[0]  # Remove batch dimension\n",
    "            \n",
    "#             # If multi-class segmentation, take argmax or the fingerprint class\n",
    "#             if mask.shape[-1] > 1:\n",
    "#                 mask = np.argmax(mask, axis=-1)  # Convert to class indices\n",
    "#                 # Assuming fingerprint class has index 1 (adjust if different)\n",
    "#                 binary_mask = (mask == 1).astype(np.uint8) * 255\n",
    "#             else:\n",
    "#                 # For binary segmentation\n",
    "#                 binary_mask = (mask[:, :, 0] > 0.5).astype(np.uint8) * 255\n",
    "#         else:\n",
    "#             # Handle unexpected output format\n",
    "#             raise ValueError(\"Unexpected model output format\")\n",
    "            \n",
    "#         # Resize back to original image dimensions if needed\n",
    "#         if model_input_shape[0] is not None and model_input_shape[1] is not None:\n",
    "#             if binary_mask.shape[:2] != image.shape[:2]:\n",
    "#                 binary_mask = cv2.resize(binary_mask, (image.shape[1], image.shape[0]))\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Segmentation model error: {e}. Using fallback thresholding method.\")\n",
    "#         # Fall back to the thresholding approach\n",
    "#         if len(image.shape) == 2:\n",
    "#             gray = image\n",
    "#         else:\n",
    "#             gray = image[:,:,0]\n",
    "        \n",
    "#         # Make sure image is in the right range for thresholding\n",
    "#         if gray.max() <= 1.0:\n",
    "#             gray_uint8 = (gray * 255).astype(np.uint8)\n",
    "#         else:\n",
    "#             gray_uint8 = gray.astype(np.uint8)\n",
    "            \n",
    "#         # Apply adaptive thresholding to get a binary mask\n",
    "#         binary_mask = cv2.adaptiveThreshold(\n",
    "#             gray_uint8, \n",
    "#             255, \n",
    "#             cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "#             cv2.THRESH_BINARY_INV, \n",
    "#             11, \n",
    "#             2\n",
    "#         )\n",
    "    \n",
    "#     # Clean up the mask\n",
    "#     kernel = np.ones((3, 3), np.uint8)\n",
    "#     binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "#     binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "#     # Find contours\n",
    "#     contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "#     # If no contours found, return the original image and a full-frame bounding box\n",
    "#     if not contours:\n",
    "#         h, w = image.shape[:2]\n",
    "#         return image, (0, 0, w, h)\n",
    "    \n",
    "#     # Find the largest contour\n",
    "#     largest_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "#     # Get bounding box of the largest contour\n",
    "#     x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    \n",
    "#     # Add padding\n",
    "#     x_min = max(0, x - padding)\n",
    "#     y_min = max(0, y - padding)\n",
    "#     x_max = min(original_image.shape[1], x + w + padding)\n",
    "#     y_max = min(original_image.shape[0], y + h + padding)\n",
    "    \n",
    "#     # Crop the image\n",
    "#     if len(original_image.shape) == 2:\n",
    "#         cropped_image = original_image[y_min:y_max, x_min:x_max]\n",
    "#     else:\n",
    "#         cropped_image = original_image[y_min:y_max, x_min:x_max, :]\n",
    "    \n",
    "#     return cropped_image, (x_min, y_min, x_max, y_max)\n",
    "\n",
    "def extract_fingerprint_region(image, seg_model, padding=10):\n",
    "    \"\"\"\n",
    "    Extract the fingerprint region using the segmentation model.\n",
    "    \n",
    "    Args:\n",
    "        image: Input fingerprint image (normalized between 0-1)\n",
    "        seg_model: Trained segmentation model\n",
    "        padding: Padding to add around the detected region\n",
    "        \n",
    "    Returns:\n",
    "        cropped_image: Cropped fingerprint region\n",
    "        bbox: Bounding box coordinates (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    # Ensure image has proper dimensions\n",
    "    original_image = image.copy()\n",
    "    \n",
    "    try:\n",
    "        # Prepare image for the segmentation model\n",
    "        if len(image.shape) == 2:\n",
    "            # Add channel dimension if grayscale\n",
    "            input_img = np.expand_dims(image, axis=-1)\n",
    "        else:\n",
    "            input_img = image\n",
    "            \n",
    "        # Resize if necessary based on model's expected input shape\n",
    "        # model_input_shape = seg_model.inputs[0].shape.as_list()[1:3]  \n",
    "        \n",
    "        try:\n",
    "        # Try TensorFlow tensor approach\n",
    "            model_input_shape = seg_model.inputs[0].shape.as_list()[1:3]\n",
    "        except AttributeError:\n",
    "            # Handle case where shape is a tuple\n",
    "            model_input_shape = seg_model.inputs[0].shape[1:3]\n",
    "        # Get expected height and width\n",
    "        if model_input_shape[0] is not None and model_input_shape[1] is not None:\n",
    "            # If model expects specific dimensions, resize the image\n",
    "            input_img_resized = cv2.resize(input_img, (model_input_shape[1], model_input_shape[0]))\n",
    "            # Ensure proper dimensions for model input (batch dimension)\n",
    "            input_img_resized = np.expand_dims(input_img_resized, axis=0)\n",
    "        else:\n",
    "            # If model accepts dynamic input, just add batch dimension\n",
    "            input_img_resized = np.expand_dims(input_img, axis=0)\n",
    "        \n",
    "        # Predict segmentation mask\n",
    "        prediction = seg_model.predict(input_img_resized)\n",
    "        \n",
    "        # Process the predicted mask\n",
    "        if len(prediction.shape) == 4:  # (batch, height, width, channels)\n",
    "            mask = prediction[0]  # Remove batch dimension\n",
    "            \n",
    "            # If multi-class segmentation, take argmax or the fingerprint class\n",
    "            if mask.shape[-1] > 1:\n",
    "                mask = np.argmax(mask, axis=-1)  # Convert to class indices\n",
    "                # Assuming fingerprint class has index 1 (adjust if different)\n",
    "                binary_mask = (mask == 1).astype(np.uint8) * 255\n",
    "            else:\n",
    "                # For binary segmentation\n",
    "                binary_mask = (mask[:, :, 0] > 0.5).astype(np.uint8) * 255\n",
    "        else:\n",
    "            # Handle unexpected output format\n",
    "            raise ValueError(\"Unexpected model output format\")\n",
    "            \n",
    "        # Resize back to original image dimensions if needed\n",
    "        if model_input_shape[0] is not None and model_input_shape[1] is not None:\n",
    "            if binary_mask.shape[:2] != image.shape[:2]:\n",
    "                binary_mask = cv2.resize(binary_mask, (image.shape[1], image.shape[0]))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Segmentation model error: {e}. Using fallback thresholding method.\")\n",
    "        # Fall back to the thresholding approach\n",
    "        if len(image.shape) == 2:\n",
    "            gray = image\n",
    "        else:\n",
    "            gray = image[:,:,0]\n",
    "        \n",
    "        # Make sure image is in the right range for thresholding\n",
    "        if gray.max() <= 1.0:\n",
    "            gray_uint8 = (gray * 255).astype(np.uint8)\n",
    "        else:\n",
    "            gray_uint8 = gray.astype(np.uint8)\n",
    "            \n",
    "        # Apply adaptive thresholding to get a binary mask\n",
    "        binary_mask = cv2.adaptiveThreshold(\n",
    "            gray_uint8, \n",
    "            255, \n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "            cv2.THRESH_BINARY_INV, \n",
    "            11, \n",
    "            2\n",
    "        )\n",
    "    \n",
    "    # Clean up the mask\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # If no contours found, return the original image and a full-frame bounding box\n",
    "    if not contours:\n",
    "        h, w = image.shape[:2]\n",
    "        return image, (0, 0, w, h)\n",
    "    \n",
    "    # Find the largest contour\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Get bounding box of the largest contour\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    \n",
    "    # Add padding\n",
    "    x_min = max(0, x - padding)\n",
    "    y_min = max(0, y - padding)\n",
    "    x_max = min(original_image.shape[1], x + w + padding)\n",
    "    y_max = min(original_image.shape[0], y + h + padding)\n",
    "    \n",
    "    # Crop the image\n",
    "    if len(original_image.shape) == 2:\n",
    "        cropped_image = original_image[y_min:y_max, x_min:x_max]\n",
    "    else:\n",
    "        cropped_image = original_image[y_min:y_max, x_min:x_max, :]\n",
    "    \n",
    "    return cropped_image, (x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_extracted_regions(images, seg_model=None, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize the extracted fingerprint regions.\n",
    "    \n",
    "    Args:\n",
    "        images: Array of fingerprint images\n",
    "        seg_model: Not used in this version, kept for compatibility\n",
    "        num_samples: Number of random samples to visualize\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if not already\n",
    "    if not isinstance(images, np.ndarray):\n",
    "        images = np.array(images)\n",
    "    \n",
    "    # Select random samples\n",
    "    if len(images) <= num_samples:\n",
    "        indices = list(range(len(images)))\n",
    "    else:\n",
    "        indices = np.random.choice(range(len(images)), num_samples, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*len(indices)))\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get original image\n",
    "        image = images[idx].copy()  # Make a copy to avoid modifying original\n",
    "        \n",
    "        # Handle different input formats\n",
    "        if len(image.shape) == 3 and (image.shape[0] == 1 or image.shape[-1] == 1):\n",
    "            image = image.squeeze()  # Remove singleton dimensions\n",
    "        \n",
    "        # Normalize if needed\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        try:\n",
    "            print(f\"Processing image {idx}, shape: {image.shape}\")\n",
    "            # Extract region using classical CV methods\n",
    "            cropped, bbox = extract_fingerprint_region(image, seg_model)\n",
    "            \n",
    "            # Original image\n",
    "            plt.subplot(len(indices), 3, i*3+1)\n",
    "            plt.title('Original Image')\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Draw bounding box on original\n",
    "            plt.subplot(len(indices), 3, i*3+2)\n",
    "            plt.title('Detected Region')\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            rect = plt.Rectangle((x_min, y_min), x_max-x_min, y_max-y_min, \n",
    "                              fill=False, edgecolor='red', linewidth=2)\n",
    "            plt.gca().add_patch(rect)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Cropped and resized image\n",
    "            plt.subplot(len(indices), 3, i*3+3)\n",
    "            plt.title('Cropped Region')\n",
    "            plt.imshow(cropped, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {idx}: {e}\")\n",
    "            plt.subplot(len(indices), 3, i*3+1)\n",
    "            plt.title(f'Error: {str(e)[:20]}...')\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Leave other plots empty\n",
    "            plt.subplot(len(indices), 3, i*3+2)\n",
    "            plt.title('Error occurred')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(len(indices), 3, i*3+3)\n",
    "            plt.axis('off')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_extracted_regions(x_train, seg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_combined_pipeline(fingerprint_model, seg_model, x_val, label_val, data_dict, label_real_dict, num_samples=1):\n",
    "    \"\"\"\n",
    "    Evaluate the complete fingerprint matching pipeline with region detection.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # indices = np.random.choice(range(len(x_val)), num_samples, replace=False)\n",
    "    num_samples = min(num_samples, len(x_val))\n",
    "    indices = np.random.choice(range(len(x_val)), num_samples, replace=False)\n",
    "    \n",
    "    valid_size = len(x_val)\n",
    "    num_samples = min(num_samples, valid_size) \n",
    "    indices = np.random.choice(valid_size, num_samples, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(20, 5*num_samples))\n",
    "    for i, idx in enumerate(indices):\n",
    "        input_img = x_val[idx].copy()\n",
    "        match_key = ''.join(label_val[idx].astype(str)).zfill(6)\n",
    "        \n",
    "        # Get matching real fingerprint\n",
    "        matched_img = data_dict['real'][0][label_real_dict[match_key]].copy()\n",
    "        \n",
    "        # Get random non-matching fingerprint\n",
    "        while True:\n",
    "            unmatch_key, unmatch_idx = random.choice(list(label_real_dict.items()))\n",
    "            if unmatch_key != match_key:\n",
    "                unmatched_img = data_dict['real'][0][unmatch_idx].copy()\n",
    "                break\n",
    "        \n",
    "        # Process input image\n",
    "        input_normalized = input_img.astype(np.float32) / 255.0\n",
    "        input_segmented, input_bbox = extract_fingerprint_region(input_normalized, seg_model)\n",
    "        input_segmented = input_segmented.reshape(1, 90, 90, 1)\n",
    "        \n",
    "        # Process matched image\n",
    "        matched_normalized = matched_img.astype(np.float32) / 255.0\n",
    "        matched_segmented, matched_bbox = extract_fingerprint_region(matched_normalized, seg_model)\n",
    "        matched_segmented = matched_segmented.reshape(1, 90, 90, 1)\n",
    "        \n",
    "        # Process unmatched image\n",
    "        unmatched_normalized = unmatched_img.astype(np.float32) / 255.0\n",
    "        unmatched_segmented, unmatched_bbox = extract_fingerprint_region(unmatched_normalized, seg_model)\n",
    "        unmatched_segmented = unmatched_segmented.reshape(1, 90, 90, 1)\n",
    "        \n",
    "        # Predict similarity\n",
    "        pred_matched = fingerprint_model.predict([input_segmented, matched_segmented])[0][0]\n",
    "        pred_unmatched = fingerprint_model.predict([input_segmented, unmatched_segmented])[0][0]\n",
    "        \n",
    "        results.append({\n",
    "            'input_id': idx,\n",
    "            'match_score': pred_matched,\n",
    "            'unmatch_score': pred_unmatched,\n",
    "            'correct': pred_matched > 0.5 and pred_unmatched < 0.5\n",
    "        })\n",
    "        \n",
    "        # Original images with bounding boxes\n",
    "        plt.subplot(num_samples, 6, i*6+1)\n",
    "        plt.title(f'Input: {label_val[idx]}')\n",
    "        plt.imshow(input_img.squeeze(), cmap='gray')\n",
    "        rect = plt.Rectangle((input_bbox[0], input_bbox[1]), \n",
    "                           input_bbox[2]-input_bbox[0], input_bbox[3]-input_bbox[1], \n",
    "                           fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        plt.subplot(num_samples, 6, i*6+2)\n",
    "        plt.title('Input Segmented')\n",
    "        plt.imshow(input_segmented.squeeze(), cmap='gray')\n",
    "        \n",
    "        # Matched pair\n",
    "        plt.subplot(num_samples, 6, i*6+3)\n",
    "        plt.title(f'Match')\n",
    "        plt.imshow(matched_img.squeeze(), cmap='gray')\n",
    "        rect = plt.Rectangle((matched_bbox[0], matched_bbox[1]), \n",
    "                           matched_bbox[2]-matched_bbox[0], matched_bbox[3]-matched_bbox[1], \n",
    "                           fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        plt.subplot(num_samples, 6, i*6+4)\n",
    "        plt.title(f'Match Segmented\\nScore: {pred_matched:.2f}')\n",
    "        plt.imshow(matched_segmented.squeeze(), cmap='gray')\n",
    "        \n",
    "        # Unmatched pair\n",
    "        plt.subplot(num_samples, 6, i*6+5)\n",
    "        plt.title(f'Unmatch')\n",
    "        plt.imshow(unmatched_img.squeeze(), cmap='gray')\n",
    "        rect = plt.Rectangle((unmatched_bbox[0], unmatched_bbox[1]), \n",
    "                           unmatched_bbox[2]-unmatched_bbox[0], unmatched_bbox[3]-unmatched_bbox[1], \n",
    "                           fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        plt.subplot(num_samples, 6, i*6+6)\n",
    "        plt.title(f'Unmatch Segmented\\nScore: {pred_unmatched:.2f}')\n",
    "        plt.imshow(unmatched_segmented.squeeze(), cmap='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct_predictions = sum(1 for r in results if r['correct'])\n",
    "    accuracy = correct_predictions / len(results)\n",
    "    print(f\"Pipeline accuracy on sample: {accuracy:.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_combined_pipeline(model, seg_model, x_val, label_val, data_dict, label_real_dict)\n",
    "\n",
    "# Compare performance between original and enhanced models\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['accuracy'], label='Original')\n",
    "plt.plot(history.history['accuracy'], label='With Segmentation') \n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['val_accuracy'], label='With Segmentation')  \n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_folder_structure():\n",
    "    \"\"\"Create the necessary folder structure for storing models\"\"\"\n",
    "    os.makedirs('../fingerprint_models', exist_ok=True)\n",
    "    os.makedirs('../fingerprint_results', exist_ok=True)\n",
    "    os.makedirs('../fingerprint_models/recognition', exist_ok=True)\n",
    "    os.makedirs('../fingerprint_models/segmentation', exist_ok=True)\n",
    "    print(\"Folder structure created successfully.\")\n",
    "\n",
    "def store_model(model, model_type, model_name, version=None, accuracy=None):\n",
    "    if model_type not in ['recognition', 'segmentation']:\n",
    "        raise ValueError(\"model_type must be either 'recognition' or 'segmentation'\")\n",
    "\n",
    "    model_id = f\"{model_name.replace(' ', '_').lower()}\"\n",
    "    \n",
    "    folder_path = f\"../fingerprint_models/{model_type}\"\n",
    "    model_path = f\"{folder_path}/{model_id}.keras\"  \n",
    "\n",
    "    model_absolute_path = f\"fingerprint-recognition/fingperint_models/{model_type}/{model_id}.keras\"\n",
    "    save_model(model, model_path)  # Corrected here\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Create metadata\n",
    "    now = datetime.now().isoformat()\n",
    "    metadata = {\n",
    "        \"name\": model_name,\n",
    "        \"path\": model_absolute_path,\n",
    "        \"accuracy\": accuracy if accuracy is not None else 0.0,\n",
    "        \"validationScore\"\n",
    "        \"createdAt\": now,\n",
    "        \"updatedAt\": now\n",
    "    }\n",
    "    \n",
    "    if version is not None:\n",
    "        metadata[\"version\"] = version\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def update_model_list(model_type, model_metadata):\n",
    "    if model_type not in ['recognition', 'segmentation']:\n",
    "        raise ValueError(\"model_type must be either 'recognition' or 'segmentation'\")\n",
    "    \n",
    "    json_path = f\"../fingerprint_results/fingerprint_{model_type}_models.json\"\n",
    "    \n",
    "    # Check if the file exists and is not empty\n",
    "    if os.path.exists(json_path) and os.path.getsize(json_path) > 0:\n",
    "        with open(json_path, 'r') as f:\n",
    "            try:\n",
    "                models = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Corrupt JSON file detected at {json_path}. Resetting file.\")\n",
    "                models = []  # Reset to empty list if corruption occurs\n",
    "    else:\n",
    "        models = []\n",
    "    \n",
    "    # Add new model metadata\n",
    "    models.append(model_metadata)\n",
    "    \n",
    "    # Save the updated list\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(models, f, indent=4)  # Fixed incorrect json.dumps usage\n",
    "    \n",
    "    print(f\"Updated {json_path} with new model information\")\n",
    "\n",
    "def calculate_accuracy(model, test_generator, num_batches=None):\n",
    "    results = model.evaluate(test_generator, steps=num_batches)\n",
    "    return results[1]  # Assuming accuracy is the second metric\n",
    "\n",
    "def calculate_iou_accuracy(seg_model, x_val, y_val):\n",
    "    y_pred = seg_model.predict(x_val)\n",
    "    \n",
    "    iou_scores = []\n",
    "    for i in range(len(y_val)):\n",
    "        true = y_val[i].squeeze()\n",
    "        pred = (y_pred[i].squeeze() > 0.5).astype(np.float32)\n",
    "        \n",
    "        intersection = np.sum(true * pred)\n",
    "        union = np.sum(true) + np.sum(pred) - intersection\n",
    "        iou = intersection / (union + 1e-7)\n",
    "        iou_scores.append(iou)\n",
    "    \n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    return mean_iou\n",
    "\n",
    "create_folder_structure()\n",
    "x_images, y_masks = load_segmentation_dataset()\n",
    "x_images = x_images.astype(np.float32) / 255.0\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_images, y_masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# if 'seg_model' in locals():\n",
    "iou_accuracy = calculate_iou_accuracy(seg_model, x_val, y_val)\n",
    "\n",
    "seg_metadata = store_model(\n",
    "    seg_model,\n",
    "    model_type='segmentation',\n",
    "    model_name='UNet Segmentation',\n",
    "    version='1.0',\n",
    "    accuracy=float(iou_accuracy)\n",
    ")\n",
    "\n",
    "update_model_list('segmentation', seg_metadata)\n",
    "\n",
    "# if 'model' in locals():\n",
    "recognition_accuracy = calculate_accuracy(model, val_gen)\n",
    "\n",
    "rec_metadata = store_model(\n",
    "    model,\n",
    "    model_type='recognition',\n",
    "    model_name='Siamese Network',\n",
    "    version='1.0',\n",
    "    accuracy=float(recognition_accuracy)\n",
    ")\n",
    "    \n",
    "update_model_list('recognition', rec_metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
